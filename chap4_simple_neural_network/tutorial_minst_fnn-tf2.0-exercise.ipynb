{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "\n",
    "def mnist_dataset():\n",
    "    (x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
    "    #normalize\n",
    "    x = x/255.0\n",
    "    x_test = x_test/255.0\n",
    "    \n",
    "    return (x, y), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip([1, 2, 3, 4], ['a', 'b', 'c', 'd'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModel:\n",
    "    def __init__(self):\n",
    "        ####################\n",
    "        '''声明模型对应的参数'''\n",
    "        ####################\n",
    "        self.W1 = tf.Variable(shape=[28 * 28, 512], dtype=tf.float32,\n",
    "            initial_value=tf.random.normal(shape=[28 * 28, 512]))\n",
    "        self.b1 = tf.Variable(shape=[512], dtype=tf.float32, initial_value=tf.zeros(shape=[512]))\n",
    "\n",
    "        self.W2 = tf.Variable(shape=[512, 10], dtype=tf.float32,\n",
    "            initial_value=tf.random.normal(shape=[512, 10]))\n",
    "        self.b2 = tf.Variable(shape=[10], dtype=tf.float32, initial_value=tf.zeros(shape=[10]))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        ####################\n",
    "        '''实现模型函数体，返回未归一化的logits'''\n",
    "        ####################\n",
    "        x = tf.reshape(x, [x.shape[0], -1])\n",
    "        logits = tf.matmul(tf.nn.relu(tf.matmul(x, self.W1) + self.b1), self.W2) + self.b2\n",
    "        return logits\n",
    "        \n",
    "model = myModel()\n",
    "\n",
    "optimizer = optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels))\n",
    "\n",
    "@tf.function\n",
    "def compute_accuracy(logits, labels):\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = compute_loss(logits, y)\n",
    "\n",
    "    # compute gradient\n",
    "    trainable_vars = [model.W1, model.W2, model.b1, model.b2]\n",
    "    grads = tape.gradient(loss, trainable_vars)\n",
    "    for g, v in zip(grads, trainable_vars):\n",
    "        v.assign_sub(0.01*g)\n",
    "\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "\n",
    "    # loss and accuracy is scalar tensor\n",
    "    return loss, accuracy\n",
    "\n",
    "@tf.function\n",
    "def test(model, x, y):\n",
    "    logits = model(x)\n",
    "    loss = compute_loss(logits, y)\n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : loss 207.4923 ; accuracy 0.104616664\n",
      "epoch 1 : loss 174.65172 ; accuracy 0.10193333\n",
      "epoch 2 : loss 156.26036 ; accuracy 0.10405\n",
      "epoch 3 : loss 143.55434 ; accuracy 0.10898333\n",
      "epoch 4 : loss 134.03447 ; accuracy 0.11523333\n",
      "epoch 5 : loss 126.4726 ; accuracy 0.123\n",
      "epoch 6 : loss 120.24098 ; accuracy 0.13048333\n",
      "epoch 7 : loss 114.9113 ; accuracy 0.13881667\n",
      "epoch 8 : loss 110.231094 ; accuracy 0.14845\n",
      "epoch 9 : loss 106.01011 ; accuracy 0.15816666\n",
      "epoch 10 : loss 102.11651 ; accuracy 0.16766667\n",
      "epoch 11 : loss 98.479515 ; accuracy 0.17838334\n",
      "epoch 12 : loss 95.060684 ; accuracy 0.18933333\n",
      "epoch 13 : loss 91.8387 ; accuracy 0.2008\n",
      "epoch 14 : loss 88.79859 ; accuracy 0.21228333\n",
      "epoch 15 : loss 85.93025 ; accuracy 0.2239\n",
      "epoch 16 : loss 83.22372 ; accuracy 0.23515\n",
      "epoch 17 : loss 80.66999 ; accuracy 0.2469\n",
      "epoch 18 : loss 78.258415 ; accuracy 0.25755\n",
      "epoch 19 : loss 75.9789 ; accuracy 0.26898333\n",
      "epoch 20 : loss 73.828125 ; accuracy 0.28013334\n",
      "epoch 21 : loss 71.79453 ; accuracy 0.29045\n",
      "epoch 22 : loss 69.86936 ; accuracy 0.30076668\n",
      "epoch 23 : loss 68.04356 ; accuracy 0.31075\n",
      "epoch 24 : loss 66.30842 ; accuracy 0.31998333\n",
      "epoch 25 : loss 64.656784 ; accuracy 0.32918334\n",
      "epoch 26 : loss 63.085876 ; accuracy 0.33836666\n",
      "epoch 27 : loss 61.594086 ; accuracy 0.34813333\n",
      "epoch 28 : loss 60.17651 ; accuracy 0.35715\n",
      "epoch 29 : loss 58.827267 ; accuracy 0.36551666\n",
      "epoch 30 : loss 57.541122 ; accuracy 0.37338334\n",
      "epoch 31 : loss 56.31421 ; accuracy 0.3811\n",
      "epoch 32 : loss 55.14305 ; accuracy 0.38908333\n",
      "epoch 33 : loss 54.026268 ; accuracy 0.397\n",
      "epoch 34 : loss 52.95952 ; accuracy 0.40476668\n",
      "epoch 35 : loss 51.93935 ; accuracy 0.41165\n",
      "epoch 36 : loss 50.962708 ; accuracy 0.41838333\n",
      "epoch 37 : loss 50.027946 ; accuracy 0.42496666\n",
      "epoch 38 : loss 49.13189 ; accuracy 0.43131667\n",
      "epoch 39 : loss 48.272343 ; accuracy 0.4378\n",
      "epoch 40 : loss 47.447205 ; accuracy 0.44366667\n",
      "epoch 41 : loss 46.65419 ; accuracy 0.44975\n",
      "epoch 42 : loss 45.89171 ; accuracy 0.45598334\n",
      "epoch 43 : loss 45.158333 ; accuracy 0.46096668\n",
      "epoch 44 : loss 44.4518 ; accuracy 0.46651667\n",
      "epoch 45 : loss 43.771576 ; accuracy 0.47205\n",
      "epoch 46 : loss 43.116615 ; accuracy 0.47748333\n",
      "epoch 47 : loss 42.485985 ; accuracy 0.48266667\n",
      "epoch 48 : loss 41.877968 ; accuracy 0.48801666\n",
      "epoch 49 : loss 41.291805 ; accuracy 0.49286667\n",
      "epoch 50 : loss 40.7263 ; accuracy 0.49768335\n",
      "epoch 51 : loss 40.180107 ; accuracy 0.50268334\n",
      "epoch 52 : loss 39.65252 ; accuracy 0.50776666\n",
      "epoch 53 : loss 39.142918 ; accuracy 0.5114833\n",
      "epoch 54 : loss 38.649956 ; accuracy 0.51596665\n",
      "epoch 55 : loss 38.17267 ; accuracy 0.52066666\n",
      "epoch 56 : loss 37.71026 ; accuracy 0.5244167\n",
      "epoch 57 : loss 37.26201 ; accuracy 0.52781665\n",
      "epoch 58 : loss 36.827667 ; accuracy 0.5323167\n",
      "epoch 59 : loss 36.406734 ; accuracy 0.5362333\n",
      "epoch 60 : loss 35.99835 ; accuracy 0.54013336\n",
      "epoch 61 : loss 35.601833 ; accuracy 0.5438167\n",
      "epoch 62 : loss 35.21666 ; accuracy 0.5469\n",
      "epoch 63 : loss 34.842094 ; accuracy 0.55043334\n",
      "epoch 64 : loss 34.47747 ; accuracy 0.5535333\n",
      "epoch 65 : loss 34.12233 ; accuracy 0.55665\n",
      "epoch 66 : loss 33.77637 ; accuracy 0.55975\n",
      "epoch 67 : loss 33.43978 ; accuracy 0.56313336\n",
      "epoch 68 : loss 33.11263 ; accuracy 0.56656665\n",
      "epoch 69 : loss 32.794342 ; accuracy 0.56956667\n",
      "epoch 70 : loss 32.484276 ; accuracy 0.5725833\n",
      "epoch 71 : loss 32.18221 ; accuracy 0.5754833\n",
      "epoch 72 : loss 31.887842 ; accuracy 0.57805\n",
      "epoch 73 : loss 31.600891 ; accuracy 0.58096665\n",
      "epoch 74 : loss 31.321184 ; accuracy 0.5840167\n",
      "epoch 75 : loss 31.048494 ; accuracy 0.5869667\n",
      "epoch 76 : loss 30.782583 ; accuracy 0.5897833\n",
      "epoch 77 : loss 30.52323 ; accuracy 0.5924\n",
      "epoch 78 : loss 30.270248 ; accuracy 0.59493333\n",
      "epoch 79 : loss 30.023458 ; accuracy 0.59765\n",
      "epoch 80 : loss 29.782658 ; accuracy 0.6002333\n",
      "epoch 81 : loss 29.547537 ; accuracy 0.6028\n",
      "epoch 82 : loss 29.317734 ; accuracy 0.60518336\n",
      "epoch 83 : loss 29.092981 ; accuracy 0.6077333\n",
      "epoch 84 : loss 28.873125 ; accuracy 0.6101\n",
      "epoch 85 : loss 28.657948 ; accuracy 0.6120667\n",
      "epoch 86 : loss 28.447165 ; accuracy 0.61395\n",
      "epoch 87 : loss 28.240583 ; accuracy 0.6160167\n",
      "epoch 88 : loss 28.038124 ; accuracy 0.61793333\n",
      "epoch 89 : loss 27.839691 ; accuracy 0.6200167\n",
      "epoch 90 : loss 27.645184 ; accuracy 0.6220833\n",
      "epoch 91 : loss 27.45453 ; accuracy 0.62415\n",
      "epoch 92 : loss 27.267612 ; accuracy 0.6260333\n",
      "epoch 93 : loss 27.084307 ; accuracy 0.62775\n",
      "epoch 94 : loss 26.904541 ; accuracy 0.62985\n",
      "epoch 95 : loss 26.728231 ; accuracy 0.6318\n",
      "epoch 96 : loss 26.5553 ; accuracy 0.6336833\n",
      "epoch 97 : loss 26.3857 ; accuracy 0.63545\n",
      "epoch 98 : loss 26.219362 ; accuracy 0.63738334\n",
      "epoch 99 : loss 26.056206 ; accuracy 0.63921666\n",
      "epoch 100 : loss 25.896109 ; accuracy 0.64086664\n",
      "epoch 101 : loss 25.738901 ; accuracy 0.6426833\n",
      "epoch 102 : loss 25.584488 ; accuracy 0.6441333\n",
      "epoch 103 : loss 25.432783 ; accuracy 0.64593333\n",
      "epoch 104 : loss 25.283747 ; accuracy 0.6476167\n",
      "epoch 105 : loss 25.137363 ; accuracy 0.64945\n",
      "epoch 106 : loss 24.99364 ; accuracy 0.6512\n",
      "epoch 107 : loss 24.852573 ; accuracy 0.6528\n",
      "epoch 108 : loss 24.714077 ; accuracy 0.6543667\n",
      "epoch 109 : loss 24.578058 ; accuracy 0.6559833\n",
      "epoch 110 : loss 24.444416 ; accuracy 0.65758336\n",
      "epoch 111 : loss 24.31307 ; accuracy 0.65931666\n",
      "epoch 112 : loss 24.183975 ; accuracy 0.66076666\n",
      "epoch 113 : loss 24.057026 ; accuracy 0.6622667\n",
      "epoch 114 : loss 23.932167 ; accuracy 0.6636\n",
      "epoch 115 : loss 23.8093 ; accuracy 0.66478336\n",
      "epoch 116 : loss 23.68833 ; accuracy 0.66606665\n",
      "epoch 117 : loss 23.569166 ; accuracy 0.66726667\n",
      "epoch 118 : loss 23.451742 ; accuracy 0.66871667\n",
      "epoch 119 : loss 23.336014 ; accuracy 0.66971666\n",
      "epoch 120 : loss 23.221964 ; accuracy 0.6710167\n",
      "epoch 121 : loss 23.10958 ; accuracy 0.67233336\n",
      "epoch 122 : loss 22.99885 ; accuracy 0.67365\n",
      "epoch 123 : loss 22.889757 ; accuracy 0.67478335\n",
      "epoch 124 : loss 22.782288 ; accuracy 0.67595\n",
      "epoch 125 : loss 22.676395 ; accuracy 0.6770333\n",
      "epoch 126 : loss 22.57205 ; accuracy 0.67843336\n",
      "epoch 127 : loss 22.469227 ; accuracy 0.6795\n",
      "epoch 128 : loss 22.367887 ; accuracy 0.68088335\n",
      "epoch 129 : loss 22.26801 ; accuracy 0.6821333\n",
      "epoch 130 : loss 22.169563 ; accuracy 0.6832333\n",
      "epoch 131 : loss 22.072508 ; accuracy 0.68448335\n",
      "epoch 132 : loss 21.976809 ; accuracy 0.68565\n",
      "epoch 133 : loss 21.882462 ; accuracy 0.68695\n",
      "epoch 134 : loss 21.789442 ; accuracy 0.6881833\n",
      "epoch 135 : loss 21.697721 ; accuracy 0.6893167\n",
      "epoch 136 : loss 21.607262 ; accuracy 0.6903833\n",
      "epoch 137 : loss 21.518045 ; accuracy 0.69151664\n",
      "epoch 138 : loss 21.430021 ; accuracy 0.69241667\n",
      "epoch 139 : loss 21.343155 ; accuracy 0.69346666\n",
      "epoch 140 : loss 21.257404 ; accuracy 0.6942833\n",
      "epoch 141 : loss 21.17276 ; accuracy 0.69526666\n",
      "epoch 142 : loss 21.089205 ; accuracy 0.69623333\n",
      "epoch 143 : loss 21.006712 ; accuracy 0.6971167\n",
      "epoch 144 : loss 20.925287 ; accuracy 0.69805\n",
      "epoch 145 : loss 20.844908 ; accuracy 0.69893336\n",
      "epoch 146 : loss 20.765554 ; accuracy 0.69991666\n",
      "epoch 147 : loss 20.687208 ; accuracy 0.70085\n",
      "epoch 148 : loss 20.609848 ; accuracy 0.70201665\n",
      "epoch 149 : loss 20.533445 ; accuracy 0.7028\n",
      "epoch 150 : loss 20.457977 ; accuracy 0.7035\n",
      "epoch 151 : loss 20.383396 ; accuracy 0.70435\n",
      "epoch 152 : loss 20.30969 ; accuracy 0.70515\n",
      "epoch 153 : loss 20.236845 ; accuracy 0.70605\n",
      "epoch 154 : loss 20.164843 ; accuracy 0.70696664\n",
      "epoch 155 : loss 20.093666 ; accuracy 0.70783335\n",
      "epoch 156 : loss 20.023312 ; accuracy 0.70853335\n",
      "epoch 157 : loss 19.953762 ; accuracy 0.7093833\n",
      "epoch 158 : loss 19.884985 ; accuracy 0.7101167\n",
      "epoch 159 : loss 19.816975 ; accuracy 0.7107667\n",
      "epoch 160 : loss 19.749712 ; accuracy 0.7116333\n",
      "epoch 161 : loss 19.683191 ; accuracy 0.7125\n",
      "epoch 162 : loss 19.617388 ; accuracy 0.71306664\n",
      "epoch 163 : loss 19.552275 ; accuracy 0.71381664\n",
      "epoch 164 : loss 19.487833 ; accuracy 0.71461666\n",
      "epoch 165 : loss 19.424025 ; accuracy 0.71541667\n",
      "epoch 166 : loss 19.360842 ; accuracy 0.7159333\n",
      "epoch 167 : loss 19.298267 ; accuracy 0.71655\n",
      "epoch 168 : loss 19.236292 ; accuracy 0.71715\n",
      "epoch 169 : loss 19.174904 ; accuracy 0.71791667\n",
      "epoch 170 : loss 19.114096 ; accuracy 0.7184833\n",
      "epoch 171 : loss 19.053858 ; accuracy 0.7191\n",
      "epoch 172 : loss 18.994188 ; accuracy 0.7198667\n",
      "epoch 173 : loss 18.93508 ; accuracy 0.7206333\n",
      "epoch 174 : loss 18.876545 ; accuracy 0.7213\n",
      "epoch 175 : loss 18.81856 ; accuracy 0.72208333\n",
      "epoch 176 : loss 18.761126 ; accuracy 0.7227\n",
      "epoch 177 : loss 18.704231 ; accuracy 0.72315\n",
      "epoch 178 : loss 18.647867 ; accuracy 0.72398335\n",
      "epoch 179 : loss 18.592035 ; accuracy 0.7247\n",
      "epoch 180 : loss 18.536724 ; accuracy 0.72541666\n",
      "epoch 181 : loss 18.481932 ; accuracy 0.7259833\n",
      "epoch 182 : loss 18.427656 ; accuracy 0.7266\n",
      "epoch 183 : loss 18.37389 ; accuracy 0.7273\n",
      "epoch 184 : loss 18.320633 ; accuracy 0.72798336\n",
      "epoch 185 : loss 18.26788 ; accuracy 0.72856665\n",
      "epoch 186 : loss 18.215626 ; accuracy 0.72925\n",
      "epoch 187 : loss 18.163866 ; accuracy 0.72968334\n",
      "epoch 188 : loss 18.112589 ; accuracy 0.73031664\n",
      "epoch 189 : loss 18.06179 ; accuracy 0.73106664\n",
      "epoch 190 : loss 18.011452 ; accuracy 0.7316\n",
      "epoch 191 : loss 17.961565 ; accuracy 0.73223335\n",
      "epoch 192 : loss 17.912113 ; accuracy 0.7327\n",
      "epoch 193 : loss 17.863092 ; accuracy 0.7331333\n",
      "epoch 194 : loss 17.814487 ; accuracy 0.73363334\n",
      "epoch 195 : loss 17.766306 ; accuracy 0.73435\n",
      "epoch 196 : loss 17.718544 ; accuracy 0.7349\n",
      "epoch 197 : loss 17.671204 ; accuracy 0.73551667\n",
      "epoch 198 : loss 17.62428 ; accuracy 0.73606664\n",
      "epoch 199 : loss 17.57778 ; accuracy 0.7366167\n",
      "epoch 200 : loss 17.531696 ; accuracy 0.73728335\n",
      "epoch 201 : loss 17.486036 ; accuracy 0.73798335\n",
      "epoch 202 : loss 17.440784 ; accuracy 0.7385333\n",
      "epoch 203 : loss 17.395939 ; accuracy 0.7392167\n",
      "epoch 204 : loss 17.351492 ; accuracy 0.7400333\n",
      "epoch 205 : loss 17.307432 ; accuracy 0.7407333\n",
      "epoch 206 : loss 17.263748 ; accuracy 0.74126667\n",
      "epoch 207 : loss 17.22043 ; accuracy 0.7417333\n",
      "epoch 208 : loss 17.177473 ; accuracy 0.7421833\n",
      "epoch 209 : loss 17.134869 ; accuracy 0.7428333\n",
      "epoch 210 : loss 17.092613 ; accuracy 0.74336666\n",
      "epoch 211 : loss 17.0507 ; accuracy 0.74378335\n",
      "epoch 212 : loss 17.00912 ; accuracy 0.7444\n",
      "epoch 213 : loss 16.967876 ; accuracy 0.7449167\n",
      "epoch 214 : loss 16.926964 ; accuracy 0.74523336\n",
      "epoch 215 : loss 16.886372 ; accuracy 0.7456667\n",
      "epoch 216 : loss 16.846094 ; accuracy 0.74615\n",
      "epoch 217 : loss 16.806131 ; accuracy 0.74651664\n",
      "epoch 218 : loss 16.766481 ; accuracy 0.7471667\n",
      "epoch 219 : loss 16.727137 ; accuracy 0.74766666\n",
      "epoch 220 : loss 16.688099 ; accuracy 0.74806666\n",
      "epoch 221 : loss 16.64936 ; accuracy 0.7485833\n",
      "epoch 222 : loss 16.61092 ; accuracy 0.7489833\n",
      "epoch 223 : loss 16.572771 ; accuracy 0.7495\n",
      "epoch 224 : loss 16.534904 ; accuracy 0.7499667\n",
      "epoch 225 : loss 16.497322 ; accuracy 0.75051665\n",
      "epoch 226 : loss 16.460018 ; accuracy 0.751\n",
      "epoch 227 : loss 16.42299 ; accuracy 0.75145\n",
      "epoch 228 : loss 16.386238 ; accuracy 0.75186664\n",
      "epoch 229 : loss 16.349752 ; accuracy 0.7524833\n",
      "epoch 230 : loss 16.313524 ; accuracy 0.7529333\n",
      "epoch 231 : loss 16.277552 ; accuracy 0.75345\n",
      "epoch 232 : loss 16.241827 ; accuracy 0.75376666\n",
      "epoch 233 : loss 16.206348 ; accuracy 0.75415\n",
      "epoch 234 : loss 16.171112 ; accuracy 0.75456667\n",
      "epoch 235 : loss 16.136122 ; accuracy 0.75505\n",
      "epoch 236 : loss 16.101372 ; accuracy 0.75555\n",
      "epoch 237 : loss 16.066866 ; accuracy 0.756\n",
      "epoch 238 : loss 16.032598 ; accuracy 0.7564167\n",
      "epoch 239 : loss 15.998569 ; accuracy 0.75695\n",
      "epoch 240 : loss 15.964771 ; accuracy 0.75726664\n",
      "epoch 241 : loss 15.931201 ; accuracy 0.7575667\n",
      "epoch 242 : loss 15.897858 ; accuracy 0.75776666\n",
      "epoch 243 : loss 15.864738 ; accuracy 0.75811666\n",
      "epoch 244 : loss 15.83184 ; accuracy 0.7586667\n",
      "epoch 245 : loss 15.799162 ; accuracy 0.75918335\n",
      "epoch 246 : loss 15.766702 ; accuracy 0.7595\n",
      "epoch 247 : loss 15.734457 ; accuracy 0.75995\n",
      "epoch 248 : loss 15.702425 ; accuracy 0.76021665\n",
      "epoch 249 : loss 15.670604 ; accuracy 0.76066667\n",
      "epoch 250 : loss 15.638989 ; accuracy 0.76115\n",
      "epoch 251 : loss 15.607585 ; accuracy 0.7616\n",
      "epoch 252 : loss 15.576391 ; accuracy 0.762\n",
      "epoch 253 : loss 15.545401 ; accuracy 0.7623\n",
      "epoch 254 : loss 15.514617 ; accuracy 0.76278335\n",
      "epoch 255 : loss 15.484029 ; accuracy 0.7632833\n",
      "epoch 256 : loss 15.453631 ; accuracy 0.7636\n",
      "epoch 257 : loss 15.423421 ; accuracy 0.76383334\n",
      "epoch 258 : loss 15.3933935 ; accuracy 0.76411664\n",
      "epoch 259 : loss 15.363547 ; accuracy 0.76448333\n",
      "epoch 260 : loss 15.333874 ; accuracy 0.76483333\n",
      "epoch 261 : loss 15.304374 ; accuracy 0.7650833\n",
      "epoch 262 : loss 15.275051 ; accuracy 0.7654833\n",
      "epoch 263 : loss 15.245902 ; accuracy 0.7658333\n",
      "epoch 264 : loss 15.216928 ; accuracy 0.7663\n",
      "epoch 265 : loss 15.18812 ; accuracy 0.7667\n",
      "epoch 266 : loss 15.159479 ; accuracy 0.767\n",
      "epoch 267 : loss 15.131001 ; accuracy 0.76736665\n",
      "epoch 268 : loss 15.102688 ; accuracy 0.7676833\n",
      "epoch 269 : loss 15.074535 ; accuracy 0.7679\n",
      "epoch 270 : loss 15.046544 ; accuracy 0.76816666\n",
      "epoch 271 : loss 15.018705 ; accuracy 0.76846665\n",
      "epoch 272 : loss 14.991022 ; accuracy 0.7687167\n",
      "epoch 273 : loss 14.9634905 ; accuracy 0.7690667\n",
      "epoch 274 : loss 14.9361105 ; accuracy 0.7693833\n",
      "epoch 275 : loss 14.908884 ; accuracy 0.76963335\n",
      "epoch 276 : loss 14.881808 ; accuracy 0.77\n",
      "epoch 277 : loss 14.854883 ; accuracy 0.7701833\n",
      "epoch 278 : loss 14.828111 ; accuracy 0.7705333\n",
      "epoch 279 : loss 14.801494 ; accuracy 0.77075\n",
      "epoch 280 : loss 14.775032 ; accuracy 0.77106667\n",
      "epoch 281 : loss 14.74872 ; accuracy 0.77141666\n",
      "epoch 282 : loss 14.722561 ; accuracy 0.77173334\n",
      "epoch 283 : loss 14.696554 ; accuracy 0.7719833\n",
      "epoch 284 : loss 14.6707 ; accuracy 0.7723\n",
      "epoch 285 : loss 14.644995 ; accuracy 0.77275\n",
      "epoch 286 : loss 14.619433 ; accuracy 0.7729833\n",
      "epoch 287 : loss 14.594021 ; accuracy 0.7733\n",
      "epoch 288 : loss 14.568754 ; accuracy 0.7736667\n",
      "epoch 289 : loss 14.543631 ; accuracy 0.77415\n",
      "epoch 290 : loss 14.518646 ; accuracy 0.7744833\n",
      "epoch 291 : loss 14.493802 ; accuracy 0.77485\n",
      "epoch 292 : loss 14.469093 ; accuracy 0.7751667\n",
      "epoch 293 : loss 14.444521 ; accuracy 0.77536666\n",
      "epoch 294 : loss 14.420082 ; accuracy 0.7756\n",
      "epoch 295 : loss 14.395775 ; accuracy 0.7759333\n",
      "epoch 296 : loss 14.371597 ; accuracy 0.7762667\n",
      "epoch 297 : loss 14.34755 ; accuracy 0.7765667\n",
      "epoch 298 : loss 14.323631 ; accuracy 0.7769333\n",
      "epoch 299 : loss 14.299837 ; accuracy 0.7772833\n",
      "epoch 300 : loss 14.276166 ; accuracy 0.77753335\n",
      "epoch 301 : loss 14.252617 ; accuracy 0.77781665\n",
      "epoch 302 : loss 14.229193 ; accuracy 0.7780667\n",
      "epoch 303 : loss 14.20589 ; accuracy 0.77846664\n",
      "epoch 304 : loss 14.182712 ; accuracy 0.7787\n",
      "epoch 305 : loss 14.159652 ; accuracy 0.77895\n",
      "epoch 306 : loss 14.136715 ; accuracy 0.77921665\n",
      "epoch 307 : loss 14.113902 ; accuracy 0.7794667\n",
      "epoch 308 : loss 14.091212 ; accuracy 0.7797833\n",
      "epoch 309 : loss 14.068644 ; accuracy 0.7802\n",
      "epoch 310 : loss 14.046199 ; accuracy 0.7805333\n",
      "epoch 311 : loss 14.023873 ; accuracy 0.7809\n",
      "epoch 312 : loss 14.001669 ; accuracy 0.7812833\n",
      "epoch 313 : loss 13.979586 ; accuracy 0.78155\n",
      "epoch 314 : loss 13.95762 ; accuracy 0.7819667\n",
      "epoch 315 : loss 13.935774 ; accuracy 0.78225\n",
      "epoch 316 : loss 13.914043 ; accuracy 0.78251666\n",
      "epoch 317 : loss 13.892431 ; accuracy 0.7826333\n",
      "epoch 318 : loss 13.870938 ; accuracy 0.78295\n",
      "epoch 319 : loss 13.8495655 ; accuracy 0.7833\n",
      "epoch 320 : loss 13.82831 ; accuracy 0.78363335\n",
      "epoch 321 : loss 13.807173 ; accuracy 0.78388333\n",
      "epoch 322 : loss 13.786149 ; accuracy 0.7842\n",
      "epoch 323 : loss 13.765237 ; accuracy 0.7846\n",
      "epoch 324 : loss 13.7444315 ; accuracy 0.7848333\n",
      "epoch 325 : loss 13.723731 ; accuracy 0.7851\n",
      "epoch 326 : loss 13.70314 ; accuracy 0.78555\n",
      "epoch 327 : loss 13.682656 ; accuracy 0.78575\n",
      "epoch 328 : loss 13.6622715 ; accuracy 0.7859833\n",
      "epoch 329 : loss 13.641992 ; accuracy 0.7862833\n",
      "epoch 330 : loss 13.621814 ; accuracy 0.7866167\n",
      "epoch 331 : loss 13.60174 ; accuracy 0.78683335\n",
      "epoch 332 : loss 13.581764 ; accuracy 0.7871\n",
      "epoch 333 : loss 13.561889 ; accuracy 0.78725\n",
      "epoch 334 : loss 13.5421095 ; accuracy 0.78748333\n",
      "epoch 335 : loss 13.522426 ; accuracy 0.7877667\n",
      "epoch 336 : loss 13.502837 ; accuracy 0.78805\n",
      "epoch 337 : loss 13.483344 ; accuracy 0.78831667\n",
      "epoch 338 : loss 13.463943 ; accuracy 0.78858334\n",
      "epoch 339 : loss 13.444634 ; accuracy 0.7887667\n",
      "epoch 340 : loss 13.425417 ; accuracy 0.78893334\n",
      "epoch 341 : loss 13.40629 ; accuracy 0.78908336\n",
      "epoch 342 : loss 13.387252 ; accuracy 0.78926665\n",
      "epoch 343 : loss 13.368302 ; accuracy 0.78945\n",
      "epoch 344 : loss 13.349438 ; accuracy 0.7897\n",
      "epoch 345 : loss 13.330661 ; accuracy 0.7898\n",
      "epoch 346 : loss 13.311967 ; accuracy 0.79003334\n",
      "epoch 347 : loss 13.293363 ; accuracy 0.79035\n",
      "epoch 348 : loss 13.274844 ; accuracy 0.7906167\n",
      "epoch 349 : loss 13.256409 ; accuracy 0.79075\n",
      "epoch 350 : loss 13.238054 ; accuracy 0.79088336\n",
      "epoch 351 : loss 13.219786 ; accuracy 0.7912\n",
      "epoch 352 : loss 13.2016 ; accuracy 0.79143333\n",
      "epoch 353 : loss 13.183494 ; accuracy 0.7916167\n",
      "epoch 354 : loss 13.165467 ; accuracy 0.79185\n",
      "epoch 355 : loss 13.147523 ; accuracy 0.79213333\n",
      "epoch 356 : loss 13.129656 ; accuracy 0.7923167\n",
      "epoch 357 : loss 13.111869 ; accuracy 0.7925\n",
      "epoch 358 : loss 13.094163 ; accuracy 0.7927833\n",
      "epoch 359 : loss 13.076534 ; accuracy 0.79303336\n",
      "epoch 360 : loss 13.058988 ; accuracy 0.79331666\n",
      "epoch 361 : loss 13.041517 ; accuracy 0.7934833\n",
      "epoch 362 : loss 13.024127 ; accuracy 0.79365\n",
      "epoch 363 : loss 13.00681 ; accuracy 0.7938167\n",
      "epoch 364 : loss 12.989571 ; accuracy 0.79403335\n",
      "epoch 365 : loss 12.972405 ; accuracy 0.79433334\n",
      "epoch 366 : loss 12.955315 ; accuracy 0.7946\n",
      "epoch 367 : loss 12.9383 ; accuracy 0.7948667\n",
      "epoch 368 : loss 12.921358 ; accuracy 0.79513335\n",
      "epoch 369 : loss 12.904489 ; accuracy 0.79535\n",
      "epoch 370 : loss 12.887692 ; accuracy 0.79553336\n",
      "epoch 371 : loss 12.870969 ; accuracy 0.79571664\n",
      "epoch 372 : loss 12.854315 ; accuracy 0.7959333\n",
      "epoch 373 : loss 12.837729 ; accuracy 0.79606664\n",
      "epoch 374 : loss 12.821215 ; accuracy 0.7962667\n",
      "epoch 375 : loss 12.804769 ; accuracy 0.79651666\n",
      "epoch 376 : loss 12.788389 ; accuracy 0.79685\n",
      "epoch 377 : loss 12.772077 ; accuracy 0.7970667\n",
      "epoch 378 : loss 12.755834 ; accuracy 0.7973167\n",
      "epoch 379 : loss 12.739658 ; accuracy 0.79755\n",
      "epoch 380 : loss 12.723548 ; accuracy 0.79765\n",
      "epoch 381 : loss 12.7075 ; accuracy 0.79791665\n",
      "epoch 382 : loss 12.691517 ; accuracy 0.79805\n",
      "epoch 383 : loss 12.675597 ; accuracy 0.79835\n",
      "epoch 384 : loss 12.6597395 ; accuracy 0.7985833\n",
      "epoch 385 : loss 12.643946 ; accuracy 0.79878336\n",
      "epoch 386 : loss 12.628217 ; accuracy 0.79898334\n",
      "epoch 387 : loss 12.612549 ; accuracy 0.7991833\n",
      "epoch 388 : loss 12.596944 ; accuracy 0.79948336\n",
      "epoch 389 : loss 12.581399 ; accuracy 0.7996\n",
      "epoch 390 : loss 12.565913 ; accuracy 0.79968333\n",
      "epoch 391 : loss 12.5504875 ; accuracy 0.7998833\n",
      "epoch 392 : loss 12.535119 ; accuracy 0.8001\n",
      "epoch 393 : loss 12.519809 ; accuracy 0.8002167\n",
      "epoch 394 : loss 12.5045595 ; accuracy 0.8003333\n",
      "epoch 395 : loss 12.489367 ; accuracy 0.80046666\n",
      "epoch 396 : loss 12.47423 ; accuracy 0.8005667\n",
      "epoch 397 : loss 12.4591465 ; accuracy 0.80081666\n",
      "epoch 398 : loss 12.444119 ; accuracy 0.80111665\n",
      "epoch 399 : loss 12.429147 ; accuracy 0.80135\n",
      "epoch 400 : loss 12.414231 ; accuracy 0.8014167\n",
      "epoch 401 : loss 12.399371 ; accuracy 0.80158335\n",
      "epoch 402 : loss 12.384567 ; accuracy 0.80183333\n",
      "epoch 403 : loss 12.369819 ; accuracy 0.8021333\n",
      "epoch 404 : loss 12.355127 ; accuracy 0.8024667\n",
      "epoch 405 : loss 12.340489 ; accuracy 0.80268335\n",
      "epoch 406 : loss 12.325909 ; accuracy 0.8028833\n",
      "epoch 407 : loss 12.311382 ; accuracy 0.80305\n",
      "epoch 408 : loss 12.296908 ; accuracy 0.8032\n",
      "epoch 409 : loss 12.282488 ; accuracy 0.80325\n",
      "epoch 410 : loss 12.268123 ; accuracy 0.80336666\n",
      "epoch 411 : loss 12.253813 ; accuracy 0.8035333\n",
      "epoch 412 : loss 12.2395525 ; accuracy 0.80373335\n",
      "epoch 413 : loss 12.225346 ; accuracy 0.8039\n",
      "epoch 414 : loss 12.211192 ; accuracy 0.80395\n",
      "epoch 415 : loss 12.197088 ; accuracy 0.80406666\n",
      "epoch 416 : loss 12.183038 ; accuracy 0.8042667\n",
      "epoch 417 : loss 12.169039 ; accuracy 0.80443335\n",
      "epoch 418 : loss 12.155089 ; accuracy 0.8046333\n",
      "epoch 419 : loss 12.141193 ; accuracy 0.8048\n",
      "epoch 420 : loss 12.12735 ; accuracy 0.8049833\n",
      "epoch 421 : loss 12.113557 ; accuracy 0.80513334\n",
      "epoch 422 : loss 12.099815 ; accuracy 0.8052667\n",
      "epoch 423 : loss 12.086122 ; accuracy 0.80541664\n",
      "epoch 424 : loss 12.072479 ; accuracy 0.80556667\n",
      "epoch 425 : loss 12.058887 ; accuracy 0.8056833\n",
      "epoch 426 : loss 12.045342 ; accuracy 0.80585\n",
      "epoch 427 : loss 12.03185 ; accuracy 0.8060667\n",
      "epoch 428 : loss 12.018405 ; accuracy 0.80626667\n",
      "epoch 429 : loss 12.005009 ; accuracy 0.8064833\n",
      "epoch 430 : loss 11.99166 ; accuracy 0.80665\n",
      "epoch 431 : loss 11.978362 ; accuracy 0.8067\n",
      "epoch 432 : loss 11.965113 ; accuracy 0.80698335\n",
      "epoch 433 : loss 11.951909 ; accuracy 0.8071667\n",
      "epoch 434 : loss 11.938754 ; accuracy 0.80736667\n",
      "epoch 435 : loss 11.925645 ; accuracy 0.80756664\n",
      "epoch 436 : loss 11.912581 ; accuracy 0.80766666\n",
      "epoch 437 : loss 11.899565 ; accuracy 0.80775\n",
      "epoch 438 : loss 11.886596 ; accuracy 0.8078833\n",
      "epoch 439 : loss 11.87367 ; accuracy 0.80808336\n",
      "epoch 440 : loss 11.860791 ; accuracy 0.8081667\n",
      "epoch 441 : loss 11.847959 ; accuracy 0.80826664\n",
      "epoch 442 : loss 11.83517 ; accuracy 0.80845\n",
      "epoch 443 : loss 11.822425 ; accuracy 0.8086333\n",
      "epoch 444 : loss 11.809727 ; accuracy 0.80878335\n",
      "epoch 445 : loss 11.797073 ; accuracy 0.8088667\n",
      "epoch 446 : loss 11.784465 ; accuracy 0.8089333\n",
      "epoch 447 : loss 11.771898 ; accuracy 0.80903333\n",
      "epoch 448 : loss 11.759373 ; accuracy 0.80918336\n",
      "epoch 449 : loss 11.746894 ; accuracy 0.80943334\n",
      "epoch 450 : loss 11.734457 ; accuracy 0.8096167\n",
      "epoch 451 : loss 11.722064 ; accuracy 0.80983335\n",
      "epoch 452 : loss 11.709711 ; accuracy 0.81006664\n",
      "epoch 453 : loss 11.697402 ; accuracy 0.81015\n",
      "epoch 454 : loss 11.685133 ; accuracy 0.81045\n",
      "epoch 455 : loss 11.672906 ; accuracy 0.8106167\n",
      "epoch 456 : loss 11.660721 ; accuracy 0.8107333\n",
      "epoch 457 : loss 11.648578 ; accuracy 0.8107833\n",
      "epoch 458 : loss 11.636475 ; accuracy 0.8111\n",
      "epoch 459 : loss 11.624411 ; accuracy 0.81125\n",
      "epoch 460 : loss 11.612386 ; accuracy 0.8113667\n",
      "epoch 461 : loss 11.600398 ; accuracy 0.8116\n",
      "epoch 462 : loss 11.588452 ; accuracy 0.81188333\n",
      "epoch 463 : loss 11.576542 ; accuracy 0.8120833\n",
      "epoch 464 : loss 11.56467 ; accuracy 0.8121833\n",
      "epoch 465 : loss 11.552835 ; accuracy 0.81228334\n",
      "epoch 466 : loss 11.541038 ; accuracy 0.81245\n",
      "epoch 467 : loss 11.529279 ; accuracy 0.8125333\n",
      "epoch 468 : loss 11.517557 ; accuracy 0.8126\n",
      "epoch 469 : loss 11.505873 ; accuracy 0.81266665\n",
      "epoch 470 : loss 11.494225 ; accuracy 0.81285\n",
      "epoch 471 : loss 11.482614 ; accuracy 0.8129333\n",
      "epoch 472 : loss 11.471038 ; accuracy 0.81315\n",
      "epoch 473 : loss 11.4595 ; accuracy 0.8132667\n",
      "epoch 474 : loss 11.447998 ; accuracy 0.8135\n",
      "epoch 475 : loss 11.436533 ; accuracy 0.8135833\n",
      "epoch 476 : loss 11.425106 ; accuracy 0.81373334\n",
      "epoch 477 : loss 11.413716 ; accuracy 0.81385\n",
      "epoch 478 : loss 11.402365 ; accuracy 0.814\n",
      "epoch 479 : loss 11.391052 ; accuracy 0.81423336\n",
      "epoch 480 : loss 11.379776 ; accuracy 0.81451666\n",
      "epoch 481 : loss 11.368535 ; accuracy 0.8146667\n",
      "epoch 482 : loss 11.35733 ; accuracy 0.81483334\n",
      "epoch 483 : loss 11.346158 ; accuracy 0.8149667\n",
      "epoch 484 : loss 11.335023 ; accuracy 0.81515\n",
      "epoch 485 : loss 11.323923 ; accuracy 0.81518334\n",
      "epoch 486 : loss 11.312859 ; accuracy 0.8153\n",
      "epoch 487 : loss 11.301828 ; accuracy 0.81546664\n",
      "epoch 488 : loss 11.290833 ; accuracy 0.81556666\n",
      "epoch 489 : loss 11.279877 ; accuracy 0.8157667\n",
      "epoch 490 : loss 11.268954 ; accuracy 0.81588334\n",
      "epoch 491 : loss 11.258067 ; accuracy 0.8160333\n",
      "epoch 492 : loss 11.247213 ; accuracy 0.81616664\n",
      "epoch 493 : loss 11.236396 ; accuracy 0.81628335\n",
      "epoch 494 : loss 11.225613 ; accuracy 0.8164167\n",
      "epoch 495 : loss 11.214864 ; accuracy 0.81655\n",
      "epoch 496 : loss 11.20415 ; accuracy 0.8167\n",
      "epoch 497 : loss 11.193471 ; accuracy 0.8167667\n",
      "epoch 498 : loss 11.182827 ; accuracy 0.81693333\n",
      "epoch 499 : loss 11.172216 ; accuracy 0.81703335\n",
      "test loss 10.970352 ; accuracy 0.8233\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = mnist_dataset()\n",
    "for epoch in range(500):\n",
    "    loss, accuracy = train_one_step(model, optimizer, \n",
    "                                    tf.constant(train_data[0], dtype=tf.float32), \n",
    "                                    tf.constant(train_data[1], dtype=tf.int64))\n",
    "    print('epoch', epoch, ': loss', loss.numpy(), '; accuracy', accuracy.numpy())\n",
    "loss, accuracy = test(model, \n",
    "                      tf.constant(test_data[0], dtype=tf.float32), \n",
    "                      tf.constant(test_data[1], dtype=tf.int64))\n",
    "\n",
    "print('test loss', loss.numpy(), '; accuracy', accuracy.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
